{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle as pickle\n",
    "import scipy.stats\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unwanted_chars = ['\\\\','.',',','/','\\'s']\n",
    "start = ['<null>']\n",
    "\n",
    "label_dict = {'neutral':0,'contradiction':1,'entailment':2}\n",
    "\n",
    "def load_data(path): #load SNLI words\n",
    "    '''\n",
    "    Constructs 4 dictionaries with the same key values across the dictionaries\n",
    "    '''\n",
    "    #data = []\n",
    "    excluded = 0\n",
    "    hypothesis = {}\n",
    "    premise = {}\n",
    "    label = {}\n",
    "    label_enc = {}\n",
    "    with open(path, 'r') as f:\n",
    "        #need this so indexing is continuous when sentences are skipped over\n",
    "        idx = 0\n",
    "        for i,line in enumerate(f):\n",
    "            obj = json.loads(line)\n",
    "            #skip these rows per readme\n",
    "            if obj[\"gold_label\"] == '-':\n",
    "                excluded += 1\n",
    "            else:\n",
    "                label[idx] = obj[\"gold_label\"]\n",
    "                label_enc[idx] = label_dict[obj[\"gold_label\"]]\n",
    "                premise[idx] = obj[\"sentence1\"]\n",
    "                hypothesis[idx] = obj[\"sentence2\"]\n",
    "                idx += 1\n",
    "    print('%s excluded' %excluded)\n",
    "    return hypothesis, premise, label, label_enc\n",
    "\n",
    "def load_embeddings(path,words_to_load,emb_dim): #load pre-trained GloVe embeddings\n",
    "    with open(path) as f:\n",
    "        loaded_embeddings = np.zeros((words_to_load, emb_dim))\n",
    "        words = {}\n",
    "        idx2words = {}\n",
    "        ordered_words = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= words_to_load: \n",
    "                break\n",
    "            s = line.split()\n",
    "            loaded_embeddings[i, :] = np.asarray(s[1:])\n",
    "            words[s[0]] = i\n",
    "            idx2words[i] = s[0]\n",
    "            ordered_words.append(s[0])\n",
    "\n",
    "    return loaded_embeddings, words, idx2words, ordered_words\n",
    "            \n",
    "def add_tokens(idx_mapping, embeddings, emb_dim):\n",
    "    '''\n",
    "    This function increases the index of the word to index mapping for GloVe so that\n",
    "    0: padding index\n",
    "    1: unk\n",
    "    2: BoS\n",
    "    '''\n",
    "    words_cnt = Counter(idx_mapping)\n",
    "    increment = Counter(dict.fromkeys(words, 3))\n",
    "    words_cnt = words_cnt + increment\n",
    "    words_cnt['<PAD_IDX>'] = 0\n",
    "    words_cnt['<UNK>'] = 1\n",
    "    words_cnt['<BoS>'] = 2\n",
    "    \n",
    "    #insert embeddings for tokens\n",
    "    '''\n",
    "    TO DO: FIX INITILIZATION\n",
    "    '''\n",
    "    #<BoS>\n",
    "    print(embeddings.shape)\n",
    "    embed = np.insert(embeddings,[0],np.random.rand(300),axis=0)\n",
    "    print(embed.shape)\n",
    "    #<UNK>\n",
    "    embed = np.insert(embed,[0],np.random.rand(300),axis=0)\n",
    "    print(embed.shape)\n",
    "    #<PAD_IDX>\n",
    "    embed = np.insert(embed,[0],np.zeros(300),axis=0)\n",
    "    print(embed.shape)\n",
    "    \n",
    "    return words_cnt, embed\n",
    "\n",
    "def clean_words(text_list): # Removes characters and makes all words lowercase\n",
    "    for i,word in enumerate(text_list):\n",
    "        for ch in unwanted_chars:\n",
    "            if ch in text_list[i]:\n",
    "                text_list[i] = text_list[i].replace(ch,'')\n",
    "            text_list[i] = text_list[i].lower()\n",
    "\n",
    "def tokenize(text_dict, idx_mapping, pad_len):\n",
    "    '''\n",
    "    text_dict: dictionary with index as key, sentence as value\n",
    "    returns dictionary with the index as key, sentenece mapped to index as value, and padded to pad_len\n",
    "    \n",
    "    QUESTION: How should we choose pad_len?  Should we truncate or should we set to the max length of \n",
    "    premise and hypothesis?\n",
    "    '''\n",
    "    tokenized_data = {}\n",
    "    for i in range(len(text_dict.keys())):\n",
    "        text_list = text_dict[i].split()\n",
    "        clean_words(text_list)\n",
    "        text_idx = []\n",
    "        for word in text_list:\n",
    "            try:\n",
    "                text_idx.append(idx_mapping[word])\n",
    "            except KeyError:\n",
    "                #UNK token\n",
    "                text_idx.append(1)\n",
    "                continue\n",
    "        #insert BoS token\n",
    "        text_idx.insert(0,2)\n",
    "        if len(text_idx) > pad_len:\n",
    "            text_idx = text_idx[:pad_len]\n",
    "        text_idx = np.concatenate((text_idx,np.zeros(max(pad_len-len(text_idx),0))))\n",
    "                                    \n",
    "        tokenized_data[i] = np.array(text_idx).astype(int)\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for lisa's laptop\n",
    "vocab_size = 50000\n",
    "emb_dim = 300\n",
    "num_classes = 3\n",
    "learning_rate = .05\n",
    "path = '/Users/Lisa/Documents/Grad School/DS-GA 1101/data/'\n",
    "glove_path = path+'glove.6B/glove.6B.300d.txt'\n",
    "text_path = path+'snli_1.0/snli_1.0_train.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "emb_dim = 300\n",
    "num_classes = 3\n",
    "#do we use learning rate anywhere?\n",
    "learning_rate = .05\n",
    "glove_path = 'glove/glove.6B.300d.txt'\n",
    "text_path = 'snli_1.0/snli_1.0_train.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785 excluded\n"
     ]
    }
   ],
   "source": [
    "hypothesis, premise, label, label_enc = load_data(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, words, idx2words, ordered_words = load_embeddings(glove_path, vocab_size, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 300)\n",
      "(50001, 300)\n",
      "(50002, 300)\n",
      "(50003, 300)\n"
     ]
    }
   ],
   "source": [
    "#modifies embeddings, words, idx2words in place to add tokens\n",
    "words, embeddings = add_tokens(words, embeddings, emb_dim)\n",
    "idx2words = {v:k for k,v in words.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_len = 10\n",
    "p_len = 15\n",
    "h_idx = tokenize(hypothesis, words, h_len)\n",
    "p_idx = tokenize(premise, words, p_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = path+'pickles/'\n",
    "with open(fp+'h_idx.pt', 'wb') as f:\n",
    "    pickle.dump(h_idx, f)\n",
    "with open(fp+'p_idx.pt', 'wb') as f:\n",
    "    pickle.dump(p_idx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample indices\n",
    "idx_list = np.array((0,1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_test_batch = torch.LongTensor(4, h_len)\n",
    "p_test_batch = torch.LongTensor(4, p_len)\n",
    "l_test_batch = torch.LongTensor(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test batch\n",
    "for i in idx_list:\n",
    "    h_test_batch[i] = torch.from_numpy(h_idx[i])\n",
    "    p_test_batch[i] = torch.from_numpy(p_idx[i])\n",
    "    l_test_batch[i] = label_enc[i]\n",
    "h_test_batch = h_test_batch.long()\n",
    "p_test_batch = p_test_batch.long()\n",
    "l_test_batch = Variable(l_test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomposableAttention(nn.Module):\n",
    "    '''\n",
    "    Starting with premise (a), we see if the hypothesis (b) is an \n",
    "    entailment, a contradiction, or neutral.\n",
    "    '''\n",
    "    def __init__(self, glove_emb, batch_size, hidden_size, h_len, p_len, num_classes, dropout=0.2):\n",
    "        super(DecomposableAttention, self).__init__()\n",
    "        self.glove = glove_emb\n",
    "        self.num_embeddings = glove_emb.shape[0]\n",
    "        self.embedding_dim = glove_emb.shape[1]\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.h_len = h_len\n",
    "        self.p_len = p_len\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embed = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        \n",
    "        '''\n",
    "        MLP LAYERS\n",
    "        '''\n",
    "        self.mlp_f = self._mlp_layers(self.hidden_size, self.hidden_size)\n",
    "        self.mlp_g = self._mlp_layers(2 * self.hidden_size, self.hidden_size)\n",
    "        self.mlp_h = self._mlp_layers(2 * self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        '''\n",
    "        Linear Layers\n",
    "        '''\n",
    "        self.project_h = nn.Linear(self.embedding_dim,self.hidden_size)\n",
    "        self.project_p = nn.Linear(self.embedding_dim,self.hidden_size)\n",
    "        self.final_linear = nn.Linear(self.hidden_size,self.num_classes)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def _mlp_layers(self, input_dim, output_dim):\n",
    "        mlp_layers = []\n",
    "        mlp_layers.append(nn.Dropout(p=self.dropout))\n",
    "        mlp_layers.append(nn.Linear(\n",
    "            input_dim, output_dim, bias=True))\n",
    "        mlp_layers.append(nn.ReLU())\n",
    "        mlp_layers.append(nn.Dropout(p=self.dropout))\n",
    "        mlp_layers.append(nn.Linear(\n",
    "            output_dim, output_dim, bias=True))\n",
    "        mlp_layers.append(nn.ReLU())   \n",
    "        #sequential runs all the layers in order\n",
    "        return nn.Sequential(*mlp_layers)  \n",
    "    \n",
    "    def forward(self, hypothesis, premise, label):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        '''\n",
    "        Get padding masks\n",
    "        Need to be LongTensors to avoid overflow with byte tensors\n",
    "        '''\n",
    "        h_mask = (hypothesis!=0).long()\n",
    "        p_mask = (premise!=0).long()\n",
    "        \n",
    "        '''\n",
    "        Embedding layer (only projection layer is trained)\n",
    "        max length = max length of of hypothesis/premise (respectively) in batch\n",
    "        Input dim: batch size x max length\n",
    "        Output dim: batch size x max length x hidden dimensions\n",
    "        '''\n",
    "        p_embedded = self.embed(Variable(premise))\n",
    "        h_embedded = self.embed(Variable(hypothesis))\n",
    "        #project from embedding dim to hidden dim\n",
    "        p_projected = self.project_p(p_embedded.view(-1,self.embedding_dim))\\\n",
    "                                         .view(self.batch_size,-1,self.hidden_size)\n",
    "        h_projected = self.project_h(h_embedded.view(-1,self.embedding_dim))\\\n",
    "                                         .view(self.batch_size,-1,self.hidden_size)\n",
    "        \n",
    "        '''\n",
    "        Relu layer (F from paper)\n",
    "        max length = max length of of hypothesis/premise (respectively) in batch\n",
    "        Input dim: batch size x max length x embedding dimensions\n",
    "        Output dim: batch size x max length x hidden dimension\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        NEW MULTILAYER PERCEPTRON\n",
    "        '''\n",
    "        F_a = self.mlp_f(p_projected.view(-1, self.hidden_size)).view(self.batch_size,-1,self.hidden_size)\n",
    "        F_b = self.mlp_f(h_projected.view(-1, self.hidden_size)).view(self.batch_size,-1,self.hidden_size)\n",
    "        \n",
    "        \n",
    "        #E dim: batch_size x max len of hypothesis x max len of premise\n",
    "        #transpose function swaps second and third axis so that F_b is batch size x hidden dim x len premise\n",
    "        E = torch.matmul(F_a,torch.transpose(F_b,1,2))  \n",
    "        \n",
    "        '''\n",
    "        Attention! \n",
    "        Given E, we reweight using the softmax and store in W_beta, W_alpha\n",
    "        W_beta dim: batch_size x len(hypothesis) x hidden dim\n",
    "        W_alpha dim: batch_size x len(premise) x hidden dim\n",
    "        \n",
    "        OLD:\n",
    "        W_beta = Variable(torch.Tensor(self.batch_size,self.p_len,self.hidden_size))\n",
    "        W_alpha = Variable(torch.Tensor(self.batch_size,self.h_len,self.hidden_size))\n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(F_b.size()[1]):\n",
    "                W_beta[i,j] = torch.mm(F.softmax(E[i,j]).view(1,-1),h_projected[i]).data\n",
    "            for k in range(F_a.size()[1]):\n",
    "                W_alpha[i,j] = torch.mm(F.softmax(E[i,:,j]).view(1,-1),p_projected[i]).data\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        mask_a = p_mask.unsqueeze(1)\n",
    "        mask_a = mask_a.expand_as(E.transpose(1, 2).contiguous()).contiguous().float()\n",
    "        mask_a = Variable(mask_a.view(-1, mask_a.size()[-1]))\n",
    "        \n",
    "        mask_b = h_mask.unsqueeze(1)\n",
    "        mask_b = mask_b.expand_as(E).contiguous().float()\n",
    "        mask_b = Variable(mask_b.view(-1, mask_b.size()[-1]))\n",
    "        \n",
    "        #alpha is softmax over premise\n",
    "        softmax_alpha = F.softmax(E.transpose(1, 2).contiguous().\\\n",
    "                                  view(-1, E.transpose(1, 2).contiguous().size()[-1]))*mask_a\n",
    "        #the +1e-13 is from allennlp. something about limiting numerical errors\n",
    "        softmax_alpha = softmax_alpha / (softmax_alpha.sum(dim=1, keepdim=True) + 1e-13)\n",
    "        softmax_alpha = softmax_alpha.view(E.transpose(1, 2).contiguous().size())\n",
    "        \n",
    "        #beta is softmax over the hypothesis\n",
    "        softmax_beta = F.softmax(E.view(-1, E.size()[-1]))*mask_b\n",
    "        softmax_beta = softmax_beta / (softmax_beta.sum(dim=1, keepdim=True) + 1e-13)\n",
    "        softmax_beta = softmax_beta.view(E.size())\n",
    "        \n",
    "        W_beta = torch.bmm(softmax_beta,h_projected)\n",
    "        W_alpha = torch.bmm(softmax_alpha,p_projected)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Compare\n",
    "        Open items:\n",
    "        1) Check that we're concatenating along the right dimensions.  Based on AllenNLP and libowen, \n",
    "            concatenated input should be batch size x len(hypothesis/premise) x (2 * embedding dim)\n",
    "        \n",
    "        Output:\n",
    "        v1 dim: batch_size x len(hypothesis) x compare_dim\n",
    "        v2 dim: batch_size x len(premise) x compare_dim\n",
    "        '''\n",
    "        #dim: batch size x len(hypotheis/premise) x (2* embedding dim)\n",
    "        cat_p_beta = torch.cat((p_projected,W_beta),2)\n",
    "        cat_h_alpha = torch.cat((h_projected,W_alpha),2)\n",
    "        \n",
    "        '''\n",
    "        NEW MULTILAYER PERCEPTRON\n",
    "        \n",
    "        TO DO: MASKING PADDING\n",
    "        '''\n",
    "        v_a = self.mlp_g(cat_p_beta.view(-1, 2*self.hidden_size)).view(self.batch_size,-1,self.hidden_size)\n",
    "        v_b = self.mlp_g(cat_h_alpha.view(-1, 2*self.hidden_size)).view(self.batch_size,-1,self.hidden_size)\n",
    "        '''\n",
    "        Aggregate\n",
    "        Given:\n",
    "        v_a = output of relu activation on the concatenation of a (premise) and beta\n",
    "        v_b = output of relu activation on the concatenation of b (hypothesis) and alpha\n",
    "        '''\n",
    "        v1 = torch.sum(v_a, dim=1)\n",
    "        v2 = torch.sum(v_b, dim=1)\n",
    "        \n",
    "        H = self.mlp_h(torch.cat((v1,v2),1))\n",
    "        H = self.final_linear(H)\n",
    "        out = F.softmax(H)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(self.glove))\n",
    "        #does not train embedded weights\n",
    "        self.embed.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_mask = (h_test_batch!=0).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-592-49eb42dd8897>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_test_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_test_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml_test_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml_test_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Some notes on this function call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "'''TESTING FOR SINGLE BATCH'''\n",
    "'''This should go in the training loop once our batch iterator is working:'''\n",
    "da = DecomposableAttention(embeddings,batch_size,200,\\\n",
    "                           h_len,p_len,num_classes,dropout=dropout_rate)\n",
    "optimizer.zero_grad()\n",
    "out= da(h_test_batch,p_test_batch,l_test_batch)\n",
    "loss = criterion(output,l_test_batch.view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def  batch_iter(dataset_size, hypothesis, premise, label_enc, batch_size, hLen, pLen):  \n",
    "    start        = -1 * batch_size\n",
    "    order        = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start     += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)\n",
    "\n",
    "        hBatch = torch.LongTensor(batch_size, hLen)\n",
    "        pBatch = torch.LongTensor(batch_size, pLen)\n",
    "        lBatch = torch.LongTensor(batch_size, 1)\n",
    "\n",
    "        idx_list = order[start:start + batch_size]\n",
    "        i = 0\n",
    "        for idx in idx_list:\n",
    "            hBatch[i] = torch.from_numpy(hypothesis[idx])\n",
    "            pBatch[i] = torch.from_numpy(premise[idx])\n",
    "            lBatch[i] = label_enc[idx]\n",
    "            i += 1\n",
    "            \n",
    "        hBatch = hBatch.long()\n",
    "        pBatch = pBatch.long()\n",
    "        lBatch = Variable(lBatch)\n",
    "\n",
    "        yield [hBatch, pBatch, lBatch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_loop(dataset_size, batch_size, num_epochs, model, data_iter, optimizer, criterion):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    losses = []\n",
    "    total_batches = int(dataset_size / batch_size)\n",
    "    start_time = time.time()\n",
    "    while epoch <= num_epochs:\n",
    "        hypothesis, premise, label = next(data_iter) \n",
    "        #start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(hypothesis, premise, label)\n",
    "        loss = criterion(output, label.view(-1))\n",
    "        \n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % total_batches == 0:\n",
    "            epoch += 1\n",
    "            if epoch % 25 == 0:\n",
    "                print( \"Epoch:\", (epoch), \"Avg Loss:\", np.mean(losses)/(total_batches*epoch), \\\n",
    "                      \"Elapsed Time: \", (start_time - time.time()))\n",
    "                start_time = time.time()\n",
    "        step += 1\n",
    "        #print(\"runtime for single batch: %s seconds\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "dropout_rate = .2\n",
    "batch_size = 4\n",
    "hidden_size = 100\n",
    "h_len = 10\n",
    "p_len = 15\n",
    "num_epochs  = 10\n",
    "learning_rate = .05\n",
    "da = DecomposableAttention(embeddings,batch_size,hidden_size,\\\n",
    "                           h_len,p_len,num_classes,dropout=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters out embedding layer which is not tuned\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, da.parameters()), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(hypothesis)\n",
    "data_iter = batch_iter(dataset_size, h_idx, p_idx, label_enc, batch_size, h_len, p_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-590-b4c176b9488c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-581-31f450a460b8>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(dataset_size, batch_size, num_epochs, model, data_iter, optimizer, criterion)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_loop(dataset_size, batch_size, num_epochs, da, data_iter, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate minutes for a single epoch:  80.97686044971148\n"
     ]
    }
   ],
   "source": [
    "print('approximate minutes for a single epoch: ',(dataset_size/4*0.03537607192993164/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
