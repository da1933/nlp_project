{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle as pickle\n",
    "import scipy.stats\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Uncomment your path below'''\n",
    "vocab_size = 50000\n",
    "emb_dim = 300\n",
    "num_classes = 3\n",
    "learning_rate = .05\n",
    "#path = '/Users/Lisa/Documents/Grad School/DS-GA 1101/data/' #Lisa's path\n",
    "#path = '' # Dan's path\n",
    "#path = 'C:/Users/karan/Anaconda3/nlp_project-master/' # Dagsha's path\n",
    "glove_path = path+'glove.6B/glove.6B.300d.txt'\n",
    "text_path = path+'snli_1.0/snli_1.0_train.jsonl'\n",
    "train_text_path = path+'snli_1.0/snli_1.0_train.jsonl'\n",
    "dev_text_path = path+'snli_1.0/snli_1.0_dev.jsonl'\n",
    "test_text_path = path+'snli_1.0/snli_1.0_test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785 excluded\n"
     ]
    }
   ],
   "source": [
    "train_hypothesis, train_premise, train_label, train_label_enc = load_data(train_text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 excluded\n"
     ]
    }
   ],
   "source": [
    "dev_hypothesis, dev_premise, dev_label, dev_label_enc = load_data(dev_text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 excluded\n"
     ]
    }
   ],
   "source": [
    "test_hypothesis, test_premise, test_label, test_label_enc = load_data(test_text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings, words, idx2words, ordered_words = load_embeddings(glove_path, vocab_size, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#modifies embeddings, words, idx2words in place to add tokens\n",
    "words, embeddings = add_tokens_da(words, embeddings, emb_dim)\n",
    "idx2words = {v:k for k,v in words.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_len = 32\n",
    "p_len = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_h_idx = tokenize_da(train_hypothesis, words, h_len)\n",
    "train_p_idx = tokenize_da(train_premise, words, p_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_h_idx = tokenize_da(dev_hypothesis, words, h_len)\n",
    "dev_p_idx = tokenize_da(dev_premise, words, p_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_h_idx = tokenize_da(test_hypothesis, words, h_len)\n",
    "test_p_idx = tokenize_da(test_premise, words, p_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecomposableAttention(nn.Module):\n",
    "    '''\n",
    "    Starting with premise (a), we see if the hypothesis (b) is an \n",
    "    entailment, a contradiction, or neutral.\n",
    "    '''\n",
    "    def __init__(self, glove_emb, batch_size, hidden_size, h_len, p_len, num_classes, dropout=0.2):\n",
    "        super(DecomposableAttention, self).__init__()\n",
    "        self.glove = glove_emb\n",
    "        self.num_embeddings = glove_emb.shape[0]\n",
    "        self.embedding_dim = glove_emb.shape[1]\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.h_len = h_len\n",
    "        self.p_len = p_len\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embed = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        \n",
    "        '''\n",
    "        MLP LAYERS\n",
    "        '''\n",
    "        self.mlp_f = self._mlp_layers(self.hidden_size, self.hidden_size)\n",
    "        self.mlp_g = self._mlp_layers(2 * self.hidden_size, self.hidden_size)\n",
    "        self.mlp_h = self._mlp_layers(2 * self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        '''\n",
    "        Linear Layers\n",
    "        '''\n",
    "        self.project_h = nn.Linear(self.embedding_dim,self.hidden_size)\n",
    "        self.project_p = nn.Linear(self.embedding_dim,self.hidden_size)\n",
    "        self.final_linear = nn.Linear(self.hidden_size,self.num_classes)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def _mlp_layers(self, input_dim, output_dim):\n",
    "        mlp_layers = []\n",
    "        mlp_layers.append(nn.Dropout(p=self.dropout))\n",
    "        mlp_layers.append(nn.Linear(input_dim, output_dim, bias=True))\n",
    "        mlp_layers.append(nn.ReLU())\n",
    "        mlp_layers.append(nn.Dropout(p=self.dropout))\n",
    "        mlp_layers.append(nn.Linear(output_dim, output_dim, bias=True))\n",
    "        mlp_layers.append(nn.ReLU())   \n",
    "        #sequential runs all the layers in order\n",
    "        return nn.Sequential(*mlp_layers)  \n",
    "    \n",
    "    def forward(self, hypothesis, premise, label):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        '''\n",
    "        Get padding masks\n",
    "        Need to be LongTensors to avoid overflow with byte tensors\n",
    "        '''\n",
    "        h_mask = (hypothesis!=0).long()\n",
    "        p_mask = (premise!=0).long()\n",
    "        \n",
    "        '''\n",
    "        Embedding layer (only projection layer is trained)\n",
    "        max length = max length of of hypothesis/premise (respectively) in batch\n",
    "        Input dim: batch size x max length\n",
    "        Output dim: batch size x max length x hidden dimensions\n",
    "        '''\n",
    "        p_embedded = self.embed(Variable(premise))\n",
    "        h_embedded = self.embed(Variable(hypothesis))\n",
    "        #project from embedding dim to hidden dim\n",
    "        p_projected = self.project_p(p_embedded.view(-1,self.embedding_dim))\\\n",
    "                                         .view(self.batch_size,-1,self.hidden_size)\n",
    "        h_projected = self.project_h(h_embedded.view(-1,self.embedding_dim))\\\n",
    "                                         .view(self.batch_size,-1,self.hidden_size)\n",
    "        \n",
    "        '''\n",
    "        First Feed Forward Network (F)\n",
    "        max length = max length of of hypothesis/premise (respectively) in batch\n",
    "        Input dim: batch size x max length x hidden dimensions\n",
    "        Output dim: batch size x max length x hidden dimension\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        NEW MULTILAYER PERCEPTRON\n",
    "        '''\n",
    "        F_a = self.mlp_f(p_projected.view(-1, self.hidden_size)).view(self.batch_size,-1,self.hidden_size)\n",
    "        F_b = self.mlp_f(h_projected.view(-1, self.hidden_size)).view(self.batch_size,-1,self.hidden_size)\n",
    "        \n",
    "        \n",
    "        #E dim: batch_size x max len of hypothesis x max len of premise\n",
    "        #transpose function swaps second and third axis so that F_b is batch size x hidden dim x len premise\n",
    "        E = torch.matmul(F_a,torch.transpose(F_b,1,2))  \n",
    "        \n",
    "        '''\n",
    "        Attention! \n",
    "        Given E, we reweight using the softmax and store in W_beta, W_alpha\n",
    "        W_beta dim: batch_size x len(premise) x hidden dim\n",
    "        W_alpha dim: batch_size x len(hypothesis) x hidden dim\n",
    "        \n",
    "        OLD:\n",
    "        W_beta = Variable(torch.Tensor(self.batch_size,self.p_len,self.hidden_size))\n",
    "        W_alpha = Variable(torch.Tensor(self.batch_size,self.h_len,self.hidden_size))\n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(F_b.size()[1]):\n",
    "                W_beta[i,j] = torch.mm(F.softmax(E[i,j]).view(1,-1),h_projected[i]).data\n",
    "            for k in range(F_a.size()[1]):\n",
    "                W_alpha[i,j] = torch.mm(F.softmax(E[i,:,j]).view(1,-1),p_projected[i]).data\n",
    "        \n",
    "        '''\n",
    "        #p_mask is batch_size x p_len\n",
    "        mask_a = p_mask.unsqueeze(1) #unsqueeze makes it batch_size x 1 x p_len\n",
    "        mask_a = mask_a.expand_as(E.transpose(1, 2)).float() #expands it to (batch_size*h_len)x p_len\n",
    "        mask_a = Variable(mask_a.view(-1, self.p_len))\n",
    "        #mask_a.requires_grad = False\n",
    "        \n",
    "        mask_b = h_mask.unsqueeze(1) #unsqueeze makes it batch_size x 1 x h_len\n",
    "        mask_b = mask_b.expand_as(E).float()  #expands it to (batch_size*p_len)x h_len\n",
    "        mask_b = Variable(mask_b.view(-1, self.h_len))\n",
    "        #mask_b.requires_grad = False\n",
    "        \n",
    "        #alpha is softmax over premise\n",
    "        #dim: batch_size x h_len x p_len\n",
    "        softmax_alpha = F.softmax(E.transpose(1, 2).contiguous().\\\n",
    "                                  view(-1, E.transpose(1, 2).size()[-1]))*mask_a\n",
    "        #the +1e-13 is from allennlp. something about limiting numerical errors\n",
    "        softmax_alpha = softmax_alpha / (softmax_alpha.sum(dim=1, keepdim=True) + 1e-13)\n",
    "        softmax_alpha = softmax_alpha.view(E.transpose(1, 2).contiguous().size())\n",
    "        \n",
    "        #beta is softmax over the hypothesis\n",
    "        #dim: batch_size x p_len x h_len\n",
    "        softmax_beta = F.softmax(E.view(-1, E.size()[-1]))*mask_b\n",
    "        softmax_beta = softmax_beta / (softmax_beta.sum(dim=1, keepdim=True) + 1e-13)\n",
    "        softmax_beta = softmax_beta.view(E.size())\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        softmax_beta is batch_size x p_len x h_len\n",
    "        h_projected is batch size x h_len x hidden dimensions\n",
    "        so W_beta is batch_size x p_len x hidden dim\n",
    "        \n",
    "        \n",
    "        softmax_alpha is batch_size x h_len x p_len\n",
    "        p_projected is batch size x p_len x hidden dimensions\n",
    "        so W_alpha is batch size x h_len x hidden dime\n",
    "        \n",
    "        '''\n",
    "        W_beta = torch.bmm(softmax_beta,h_projected)\n",
    "        W_alpha = torch.bmm(softmax_alpha,p_projected)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Compare\n",
    "        Open items:\n",
    "        1) Check that we're concatenating along the right dimensions.  Based on AllenNLP and libowen, \n",
    "            concatenated input should be batch size x len(hypothesis/premise) x (2 * embedding dim)\n",
    "        \n",
    "        Output:\n",
    "        v1 dim: batch_size x len(hypothesis) x compare_dim\n",
    "        v2 dim: batch_size x len(premise) x compare_dim\n",
    "        '''\n",
    "        #dim: batch size x len(hypotheis/premise) x (2* hidden dim)\n",
    "        cat_p_beta = torch.cat((p_projected,W_beta),2)\n",
    "        cat_h_alpha = torch.cat((h_projected,W_alpha),2)\n",
    "        \n",
    "        '''\n",
    "        MLP with masking\n",
    "        '''\n",
    "        v_a = self.mlp_g(cat_p_beta.view(-1, 2*self.hidden_size))\n",
    "        v_a = v_a*Variable(p_mask.view(-1).unsqueeze(1).expand_as(v_a).float())\n",
    "        v_a = v_a.view(self.batch_size,-1,self.hidden_size)\n",
    "        \n",
    "        v_b = self.mlp_g(cat_h_alpha.view(-1, 2*self.hidden_size))\n",
    "        v_b = v_b*Variable(h_mask.view(-1).unsqueeze(1).expand_as(v_b).float())\n",
    "        v_b = v_b.view(self.batch_size,-1,self.hidden_size)\n",
    "        '''\n",
    "        Aggregate\n",
    "        Given:\n",
    "        v_a = output of relu activation on the concatenation of a (premise) and beta\n",
    "        v_b = output of relu activation on the concatenation of b (hypothesis) and alpha\n",
    "        '''\n",
    "        v1 = torch.sum(v_a, dim=1)\n",
    "        v2 = torch.sum(v_b, dim=1)\n",
    "        \n",
    "        H = self.mlp_h(torch.cat((v1,v2),1))\n",
    "        out = self.final_linear(H)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(self.glove))\n",
    "        #does not train embedded weights\n",
    "        self.embed.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Iterator for training our model\n",
    "#This generator gives 1 batch at a time\n",
    "def batch_iter(dataset_size, hypothesis, premise, label_enc, batch_size, hLen, pLen):  \n",
    "    start        = -1 * batch_size\n",
    "    order        = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start     += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)\n",
    "\n",
    "        hBatch = torch.LongTensor(batch_size, hLen)\n",
    "        pBatch = torch.LongTensor(batch_size, pLen)\n",
    "        lBatch = torch.LongTensor(batch_size, 1)\n",
    "\n",
    "        idx_list = order[start:start + batch_size]\n",
    "        i = 0\n",
    "        for idx in idx_list:\n",
    "            hBatch[i] = torch.from_numpy(hypothesis[idx])\n",
    "            pBatch[i] = torch.from_numpy(premise[idx])\n",
    "            lBatch[i] = label_enc[idx]\n",
    "            i += 1\n",
    "            \n",
    "        hBatch = hBatch.long()\n",
    "        pBatch = pBatch.long()\n",
    "        lBatch = Variable(lBatch)\n",
    "\n",
    "        yield [hBatch, pBatch, lBatch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Iterator for evaluating our model \n",
    "#This generator gives a list of batches that can be iterated through.\n",
    "def evaluation_iter(dataset_size, hypothesis, premise, label_enc, batch_size, hLen, pLen):\n",
    "    batches = []\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        \n",
    "        hBatch = torch.LongTensor(batch_size, hLen)\n",
    "        pBatch = torch.LongTensor(batch_size, pLen)\n",
    "        lBatch = torch.LongTensor(batch_size, 1)\n",
    "\n",
    "        idx_list = order[start:start + batch_size]\n",
    "        i = 0\n",
    "        for idx in idx_list:\n",
    "            hBatch[i] = torch.from_numpy(hypothesis[idx])\n",
    "            pBatch[i] = torch.from_numpy(premise[idx])\n",
    "            lBatch[i] = label_enc[idx]\n",
    "            i += 1\n",
    "            \n",
    "        hBatch = hBatch.long()\n",
    "        pBatch = pBatch.long()\n",
    "        lBatch = Variable(lBatch)\n",
    "        \n",
    "        if len(hBatch) ==  batch_size:\n",
    "            batches.append([hBatch, pBatch, lBatch])\n",
    "        else:\n",
    "            continue       \n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function outputs the accuracy on the dataset, we will use it during training.\n",
    "def evaluate(model, data_iter, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(data_iter)):\n",
    "        hypothesis, premise, label = data_iter[i]\n",
    "        \n",
    "        output = model(hypothesis, premise, label)      \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        lab = label.data.view(-1)\n",
    "        total += lab.size(0)\n",
    "        correct += (predicted == lab).sum()\n",
    "        \n",
    "        '''\n",
    "        print(\"\\n ouput label: \")\n",
    "        print(output)\n",
    "        print(\"\\n predicted\")\n",
    "        print(predicted)\n",
    "        print(\"\\n label\")\n",
    "        print(lab.view(-1)) \n",
    "        print(\"\\n Sum: \")\n",
    "        print((predicted == lab).sum())\n",
    "        '''\n",
    "      \n",
    "    return correct / float(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH='saved_model'\n",
    "def training_loop(dataset_size, batch_size, num_epochs, model, data_iter, dev_iter, optimizer, criterion):\n",
    "    model.train()\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    losses = []\n",
    "    total_batches = int(dataset_size / batch_size)\n",
    "    print(\"\\n total_batches: \", total_batches)\n",
    "    start_time = time.time()\n",
    "    batch_time = time.time()\n",
    "    while epoch <= num_epochs:\n",
    "        hypothesis, premise, label = next(data_iter) \n",
    "        optimizer.zero_grad()\n",
    "        output = model(hypothesis, premise, label)\n",
    "        loss = criterion(output, label.view(-1))\n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % total_batches == 0:\n",
    "            epoch += 1\n",
    "            if epoch % 25 == 0:\n",
    "                print( \"Epoch:\", (epoch), \"Avg Loss:\", np.mean(losses)/(total_batches*epoch), \\\n",
    "                       \"Evaluate Loss: \", evaluate(model, dev_iter, criterion), \"Elapsed Time: \", (start_time - time.time()))\n",
    "                start_time = time.time()\n",
    "            torch.save(model.state_dict(), PATH) # Saves model after every epoch\n",
    "        \n",
    "        step += 1\n",
    "        if step % 1000 ==0:\n",
    "            print(step,'of',total_batches,'batches trained in',(time.time()-batch_time),\\\n",
    "                  'seconds. Avg Loss:',np.mean(losses))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "dropout_rate = .2\n",
    "batch_size = 4\n",
    "hidden_size = 200\n",
    "h_len = 32\n",
    "p_len = 32\n",
    "num_epochs  = 50\n",
    "learning_rate = .05\n",
    "\n",
    "da = DecomposableAttention(embeddings,batch_size,hidden_size,h_len,p_len,num_classes,dropout=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filters out embedding layer which is not tuned\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, da.parameters()), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train the Model\n",
    "train_dataset_size = len(train_hypothesis)\n",
    "data_iter = batch_iter(train_dataset_size, train_h_idx, train_p_idx, train_label_enc, batch_size, h_len, p_len)\n",
    "\n",
    "dev_dataset_size = len(dev_hypothesis)\n",
    "dev_iter = evaluation_iter(dev_dataset_size, dev_h_idx, dev_p_idx, dev_label_enc, batch_size, h_len, p_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_loop(train_dataset_size, batch_size, num_epochs, da, data_iter, dev_iter, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Load saved model'''\n",
    "load = torch.load(PATH,map_location=lambda storage,location:storage)\n",
    "da = DecomposableAttention(embeddings,batch_size,hidden_size,h_len,p_len,num_classes,dropout=dropout_rate)\n",
    "da.load_state_dict(load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our model on the test data: 34.283388\n"
     ]
    }
   ],
   "source": [
    "#Test the Model\n",
    "test_dataset_size = len(test_hypothesis)\n",
    "test_iter = evaluation_iter(test_dataset_size, test_h_idx, test_p_idx, test_label_enc, batch_size, h_len, p_len)\n",
    "test_accuracy = evaluate(da, test_iter, criterion)\n",
    "\n",
    "print('Accuracy of our model on the test data: %f' % (100 * test_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
