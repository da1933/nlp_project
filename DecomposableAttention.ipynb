{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle as pickle\n",
    "import scipy.stats\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_chars = ['\\\\','.',',','/','\\'s']\n",
    "start = ['<null>']\n",
    "\n",
    "label_dict = {'neutral':0,'contradiction':-1,'entailment':1}\n",
    "\n",
    "def load_data(path): #load SNLI words\n",
    "    '''\n",
    "    Constructs 4 dictionaries with the same key values across the dictionaries\n",
    "    '''\n",
    "    #data = []\n",
    "    excluded = 0\n",
    "    hypothesis = {}\n",
    "    premise = {}\n",
    "    label = {}\n",
    "    label_enc = {}\n",
    "    with open(path, 'r') as f:\n",
    "        #need this so indexing is continuous when setences are skipped over\n",
    "        idx = 0\n",
    "        for i,line in enumerate(f):\n",
    "            obj = json.loads(line)\n",
    "            #skip these rows per readme\n",
    "            if obj[\"gold_label\"] == '-':\n",
    "                excluded += 1\n",
    "            else:\n",
    "                label[idx] = obj[\"gold_label\"]\n",
    "                label_enc[idx] = label_dict[obj[\"gold_label\"]]\n",
    "                premise[idx] = obj[\"sentence1\"]\n",
    "                hypothesis[idx] = obj[\"sentence2\"]\n",
    "                idx += 1\n",
    "    print('%s excluded' %excluded)\n",
    "    return hypothesis, premise, label, label_enc\n",
    "\n",
    "def load_embeddings(path,number_of_words,emb_dim): #load pre-trained GloVe embeddings\n",
    "    words_to_load = number_of_words\n",
    "\n",
    "    with open(path) as f:\n",
    "        loaded_embeddings = np.zeros((words_to_load, emb_dim))\n",
    "        words = {}\n",
    "        idx2words = {}\n",
    "        ordered_words = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= words_to_load: \n",
    "                break\n",
    "            s = line.split()\n",
    "            loaded_embeddings[i, :] = np.asarray(s[1:])\n",
    "            words[s[0]] = i\n",
    "            idx2words[i] = s[0]\n",
    "            ordered_words.append(s[0])\n",
    "\n",
    "    return loaded_embeddings, words, idx2words, ordered_words\n",
    "\n",
    "def clean_words(text_list): # Removes characters and makes all words lowercase\n",
    "    for i,word in enumerate(text_list):\n",
    "        for ch in unwanted_chars:\n",
    "            if ch in text_list[i]:\n",
    "                text_list[i] = text_list[i].replace(ch,'')\n",
    "            text_list[i] = text_list[i].lower()\n",
    "            \n",
    "def add_tokens(idx_mapping, embeddings, emb_dim):\n",
    "    '''\n",
    "    This function increases the index of the word to index mapping for GloVe so that\n",
    "    0: padding index\n",
    "    1: unk\n",
    "    2: BoS\n",
    "    '''\n",
    "    words_cnt = Counter(idx_mapping)\n",
    "    increment = Counter(dict.fromkeys(words, 3))\n",
    "    words_cnt = words_cnt + increment\n",
    "    words_cnt['<PAD_IDX>'] = 0\n",
    "    words_cnt['<UNK>'] = 1\n",
    "    words_cnt['<BoS>'] = 2\n",
    "    \n",
    "    #insert embeddings for tokens\n",
    "    '''\n",
    "    TO DO: FIX INITILIZATION\n",
    "    '''\n",
    "    #<BoS>\n",
    "    embed = np.insert(embeddings,[0],np.random.rand(300),axis=0)\n",
    "    #<UNK>\n",
    "    embed = np.insert(embeddings,[0],np.random.rand(300),axis=0)\n",
    "    #<PAD_IDX>\n",
    "    embed = np.insert(embeddings,[0],np.zeros(300),axis=0)\n",
    "    \n",
    "    return words_cnt, embed\n",
    "\n",
    "def clean_words(text_list): # Removes characters and makes all words lowercase\n",
    "    for i,word in enumerate(text_list):\n",
    "        for ch in unwanted_chars:\n",
    "            if ch in text_list[i]:\n",
    "                text_list[i] = text_list[i].replace(ch,'')\n",
    "            text_list[i] = text_list[i].lower()\n",
    "\n",
    "def tokenize(text_dict, idx_mapping, pad_len):\n",
    "    '''\n",
    "    text_dict: dictionary with index as key, sentence as value\n",
    "    returns dictionary with the index as key, setenece mapped to index as value, and padded to pad_len\n",
    "    '''\n",
    "    tokenized_data = {}\n",
    "    for i in range(len(text_dict.keys())):\n",
    "        text_list = text_dict[i].split()\n",
    "        clean_words(text_list)\n",
    "        text_idx = []\n",
    "        for word in text_list:\n",
    "            try:\n",
    "                text_idx.append(idx_mapping[word])\n",
    "            except KeyError:\n",
    "                #UNK token\n",
    "                text_idx.append(1)\n",
    "                continue\n",
    "        #insert BoS token\n",
    "        text_idx.insert(0,2)\n",
    "        if len(text_idx) > pad_len:\n",
    "            text_idx = text_idx[:pad_len]\n",
    "        text_idx = np.concatenate((text_idx,np.zeros(max(pad_len-len(text_idx),0))))\n",
    "                                    \n",
    "        tokenized_data[i] = np.array(text_idx).astype(int)\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_h = {0:hypothesis[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "emb_dim = 300\n",
    "glove_path = '/Users/Lisa/Documents/Grad School/DS-GA 1101/data/glove.6B/glove.6B.300d.txt'\n",
    "text_path = '/Users/Lisa/Documents/Grad School/DS-GA 1101/data/snli_1.0/snli_1.0_train.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785 excluded\n"
     ]
    }
   ],
   "source": [
    "hypothesis, premise, label, label_enc = load_data(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings, words, idx2words, ordered_words = load_embeddings(glove_path, vocab_size, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifies embeddings, words, idx2words in place to add tokens\n",
    "words, embeddings = add_tokens(words, embeddings, emb_dim)\n",
    "idx2words = {v:k for k,v in words.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_len = 10\n",
    "p_len = 15\n",
    "h_idx = tokenize(hypothesis, words, h_len)\n",
    "p_idx = tokenize(premise, words, p_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '/Users/Lisa/Documents/Grad School/DS-GA 1101/data/pickles/'\n",
    "with open(fp+'h_idx.pt', 'wb') as f:\n",
    "    pickle.dump(h_idx, f)\n",
    "with open(fp+'p_idx.pt', 'wb') as f:\n",
    "    pickle.dump(p_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(h_idx[i]) for i in range(len(h_idx.keys()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([len(h_idx[i]) for i in range(len(h_idx.keys()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample indices\n",
    "idx_list = np.array((0,1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_test_batch = torch.LongTensor(4, h_len)\n",
    "p_test_batch = torch.LongTensor(4, p_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test batch\n",
    "for i in idx_list:\n",
    "    h_test_batch[i] = torch.from_numpy(h_idx[i])\n",
    "    p_test_batch[i] = torch.from_numpy(p_idx[i])\n",
    "h_test_batch=h_test_batch.long()\n",
    "p_test_batch=p_test_batch.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "     2     10    902     17    791     29   2870     13     10    994\n",
       "     2     10    902     17     25     10  19304   7490     32      0\n",
       "     2     10    902     17  13080     16     10   2870      0      0\n",
       "     2     42     35   8784     25     47   1111      0      0      0\n",
       "[torch.LongTensor of size 4x10]"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBD\n",
    "class DecomposableAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, glove_emb, hidden_size, h_len, p_len, num_labels):\n",
    "        super(DecomposableAttention, self).__init__()\n",
    "        self.glove = glove_emb\n",
    "        self.num_embeddings = glove_emb.shape[0]\n",
    "        self.embedding_dim = glove_emb.shape[1]\n",
    "        self.batch_size = batch_size\n",
    "        self.compare_dim = compare_dim\n",
    "        self.h_len = h_len\n",
    "        self.p_len = p_len\n",
    "        \n",
    "        self.embed = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        \n",
    "        '''\n",
    "        Should the linear layers have a bias?  How many layers should we have in our feed forward network?\n",
    "        '''\n",
    "        \n",
    "        self.F_a = nn.Linear(self.embedding_dim*self.h_len,self.embedding_dim*self.h_len)\n",
    "        self.F_b = nn.Linear(self.embedding_dim*self.p_len,self.embedding_dim*self.p_len)\n",
    "        \n",
    "        self.G_a = nn.Linear(self.embedding_dim*2,self.compare_dim)\n",
    "        self.G_b = nn.Linear(self.embedding_dim*2,self.compare_dim)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, hypothesis, premise):\n",
    "        start_time = time.time()\n",
    "        '''\n",
    "        Embedding layer\n",
    "        max length = max length of of hypothesis/premise (respectively) in batch\n",
    "        Input dim: batch size x max length\n",
    "        Output dim: batch size x max length x embedding dimensions\n",
    "        '''\n",
    "        h_embedded = self.embed(Variable(hypothesis))\n",
    "        p_embedded = self.embed(Variable(premise))\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Relu layer (F from paper)\n",
    "        max length = max length of of hypothesis/premise (respectively) in batch\n",
    "        Input dim: batch size x max length x embedding dimensions\n",
    "        Output dim: batch size x max length x embedding dimensions\n",
    "        '''\n",
    "        F_a = self.F_a(h_embedded.view(self.batch_size,-1))\n",
    "        F_b = self.F_b(p_embedded.view(self.batch_size,-1))\n",
    "        F_a = F.relu(F_a).view(self.batch_size,-1,self.embedding_dim)\n",
    "        F_b = F.relu(F_b).view(self.batch_size,-1,self.embedding_dim)\n",
    "        #E dim: batch_size x max len of hypothesis x max len of premise\n",
    "        #transpose function swaps second and third axis so that F_b is batch size x embedding dim x len premise\n",
    "        E = torch.matmul(F_a,torch.transpose(F_b,1,2))  \n",
    "        \n",
    "        '''\n",
    "        Attention! \n",
    "        Given E, we reweight using the softmax and store in W_beta, W_alpha\n",
    "        W_beta dim: batch_size x len(hypothesis) x embedding dimensions\n",
    "        W_alpha dim: batch_size x len(premise) x embedding dimensions\n",
    "        '''\n",
    "        W_beta = Variable(torch.Tensor(self.batch_size,self.h_len,self.embedding_dim))\n",
    "        W_alpha = Variable(torch.Tensor(self.batch_size,self.p_len,self.embedding_dim))\n",
    "        '''\n",
    "        TO DO: vectorize this with softmax on dimension (should be in next release of pytorch)\n",
    "        '''\n",
    "        for i in range(self.batch_size):\n",
    "            #E[i] = torch.mm(F_a[i],torch.transpose(F_b[i],0,1))\n",
    "            for j in range(F_a.size()[1]):\n",
    "                W_beta[i,j] = torch.mm(F.softmax(E[i,j]).view(1,-1),p_embedded[i]).data\n",
    "            for k in range(F_b.size()[1]):\n",
    "                W_alpha[i,j] = torch.mm(F.softmax(E[i,:,j]).view(1,-1),h_embedded[i]).data\n",
    "        \n",
    "        '''\n",
    "        Compare\n",
    "        Open items:\n",
    "        1) Check that we're concatenating along the right dimensions.  Based on AllenNLP and libowen, \n",
    "            concatenated input should be batch size x len(hypothesis/premise) x (2 * embedding dim)\n",
    "        \n",
    "        Output:\n",
    "        v1 dim: batch_size x len(hypothesis) x compare_dim\n",
    "        v2 dim: batch_size x len(premise) x compare_dim\n",
    "        '''\n",
    "        #dim: batch size x len(hypotheis/premise) x (2* embedding dim)\n",
    "        cat_h_beta = torch.cat((h_embedded,W_beta),2)\n",
    "        cat_p_alpha = torch.cat((p_embedded,W_alpha),2)\n",
    "        G_a = self.G_a(cat_h_beta.view(-1,2*self.embedding_dim))\n",
    "        G_b = self.G_b(cat_p_alpha.view(-1,2*self.embedding_dim)).view(self.batch_size,-1,self.compare_dim)\n",
    "        \n",
    "        G_a = F.relu(G_a).view(self.batch_size,-1,self.compare_dim)\n",
    "        G_b = F.relu(G_b).view(self.batch_size,-1,self.compare_dim)\n",
    "        \n",
    "        print(\"runtime for single batch: %s seconds\" % (time.time() - start_time))\n",
    "        return cat_h_beta, cat_p_alpha, G_a, G_b\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(self.glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = DecomposableAttention(4,embeddings,100,10,15,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime for single batch: 0.02577996253967285 seconds\n"
     ]
    }
   ],
   "source": [
    "cat_h_beta, cat_p_alpha, G_a, G_b = da.forward(h_test_batch,p_test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 100])"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 15, 100])"
      ]
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 600])"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_h_beta.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 15, 600])"
      ]
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_p_alpha.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_iter(source):\n",
    "    #tbd\n",
    "    return None\n",
    "    \n",
    "def data_iter(source, batch_size):\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)   \n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        yield [source[index] for index in batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Note that the Decomposable attention paper selected batch_size=4\"\"\"\n",
    "data_iter = data_iter(train_data,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TBD\n",
    "class DecomposableAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, num_labels):\n",
    "        super(DecomposableAttention, self).__init__()\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "            \n",
    "        self.a_linear = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.b_linear = nn.Linear(embedding_dim,embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input through your layers in order\n",
    "        \n",
    "        a_relu = F.relu(self.a_linear)\n",
    "        b_relu = F.relu(self.b_linear)\n",
    "        \n",
    "        return \n",
    "\n",
    "    def init_weights(self):\n",
    "        '''tbd'''\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.linear_1, self.linear_2]\n",
    "             \n",
    "        for layer in lin_layers:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_bar = Variable(torch.Tensor(train_data[1432]['premise']))\n",
    "b_bar = Variable(torch.Tensor(train_data[1432]['hypothesis']))\n",
    "\n",
    "a_linear = nn.Linear(300,300)\n",
    "b_linear = nn.Linear(300,300)\n",
    "\n",
    "a_relu = F.relu(a_linear(a_bar))\n",
    "b_relu = F.relu(b_linear(b_bar))\n",
    "\n",
    "e_ij = torch.matmul(a_relu,torch.transpose(b_relu,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention_weights(e,a_input,b_input):\n",
    "    ''' Part 3.1\n",
    "    Takes weight matrix e_ij, a_bar, and b_bar as inputs and returns \n",
    "    attention weight matrices alpha and beta.\n",
    "    The jth row in alpha aligns with the jth word in sentence b (the hypothesis).\n",
    "    The ith row in beta aligns with the ith word in sentence a (the premise)'''\n",
    "    len_a = b_input.data.numpy().shape[0]\n",
    "    len_b = a_input.data.numpy().shape[0]\n",
    "    \n",
    "    alphas = []\n",
    "    betas = []\n",
    "    for i in range(len_a):\n",
    "        alphas.append(torch.sum(a_input*torch.transpose(torch.exp(e_ij[:,i])/ \\\n",
    "                      torch.sum(torch.exp(e_ij),dim=0)[i].view(-1,1),0,1),dim=0))\n",
    "    for j in range(len_b):\n",
    "        betas.append(torch.sum(b_bar*torch.transpose(torch.exp(e_ij[i,:])/ \\\n",
    "                    torch.sum(torch.exp(e_ij),dim=1)[i].view(-1,1),0,1),dim=0))\n",
    "    \n",
    "    alpha = torch.stack(alphas)\n",
    "    beta = torch.stack(betas)\n",
    "    \n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " self.linear_1 = nn.Linear(embedding_dim, hidden_dim) \n",
    "        self.linear_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear_3 = nn.Linear(hidden_dim, num_labels)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input through your layers in order\n",
    "\n",
    "        out = F.relu(self.linear_1(out))\n",
    "        out = F.relu(self.linear_2(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
