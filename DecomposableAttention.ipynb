{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle as pickle\n",
    "import scipy.stats\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unwanted_chars = ['\\\\','.',',','/','\\'s']\n",
    "start = ['<null>']\n",
    "\n",
    "label_dict = {'neutral':0,'contradiction':1,'entailment':2}\n",
    "\n",
    "def load_data(path): #load SNLI words\n",
    "    '''\n",
    "    Constructs 4 dictionaries with the same key values across the dictionaries\n",
    "    '''\n",
    "    #data = []\n",
    "    excluded = 0\n",
    "    hypothesis = {}\n",
    "    premise = {}\n",
    "    label = {}\n",
    "    label_enc = {}\n",
    "    with open(path, 'r') as f:\n",
    "        #need this so indexing is continuous when sentences are skipped over\n",
    "        idx = 0\n",
    "        for i,line in enumerate(f):\n",
    "            obj = json.loads(line)\n",
    "            #skip these rows per readme\n",
    "            if obj[\"gold_label\"] == '-':\n",
    "                excluded += 1\n",
    "            else:\n",
    "                label[idx] = obj[\"gold_label\"]\n",
    "                label_enc[idx] = label_dict[obj[\"gold_label\"]]\n",
    "                premise[idx] = obj[\"sentence1\"]\n",
    "                hypothesis[idx] = obj[\"sentence2\"]\n",
    "                idx += 1\n",
    "    print('%s excluded' %excluded)\n",
    "    return hypothesis, premise, label, label_enc\n",
    "\n",
    "def load_embeddings(path,words_to_load,emb_dim): #load pre-trained GloVe embeddings\n",
    "    with open(path) as f:\n",
    "        loaded_embeddings = np.zeros((words_to_load, emb_dim))\n",
    "        words = {}\n",
    "        idx2words = {}\n",
    "        ordered_words = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= words_to_load: \n",
    "                break\n",
    "            s = line.split()\n",
    "            loaded_embeddings[i, :] = np.asarray(s[1:])\n",
    "            words[s[0]] = i\n",
    "            idx2words[i] = s[0]\n",
    "            ordered_words.append(s[0])\n",
    "\n",
    "    return loaded_embeddings, words, idx2words, ordered_words\n",
    "            \n",
    "def add_tokens(idx_mapping, embeddings, emb_dim):\n",
    "    '''\n",
    "    This function increases the index of the word to index mapping for GloVe so that\n",
    "    0: padding index\n",
    "    1: unk\n",
    "    2: BoS\n",
    "    '''\n",
    "    words_cnt = Counter(idx_mapping)\n",
    "    increment = Counter(dict.fromkeys(words, 3))\n",
    "    words_cnt = words_cnt + increment\n",
    "    words_cnt['<PAD_IDX>'] = 0\n",
    "    words_cnt['<UNK>'] = 1\n",
    "    words_cnt['<BoS>'] = 2\n",
    "    \n",
    "    #insert embeddings for tokens\n",
    "    '''\n",
    "    TO DO: FIX INITILIZATION\n",
    "    '''\n",
    "    #<BoS>\n",
    "    print(embeddings.shape)\n",
    "    embed = np.insert(embeddings,[0],np.random.rand(300),axis=0)\n",
    "    print(embed.shape)\n",
    "    #<UNK>\n",
    "    embed = np.insert(embed,[0],np.random.rand(300),axis=0)\n",
    "    print(embed.shape)\n",
    "    #<PAD_IDX>\n",
    "    embed = np.insert(embed,[0],np.zeros(300),axis=0)\n",
    "    print(embed.shape)\n",
    "    \n",
    "    return words_cnt, embed\n",
    "\n",
    "def clean_words(text_list): # Removes characters and makes all words lowercase\n",
    "    for i,word in enumerate(text_list):\n",
    "        for ch in unwanted_chars:\n",
    "            if ch in text_list[i]:\n",
    "                text_list[i] = text_list[i].replace(ch,'')\n",
    "            text_list[i] = text_list[i].lower()\n",
    "\n",
    "def tokenize(text_dict, idx_mapping, pad_len):\n",
    "    '''\n",
    "    text_dict: dictionary with index as key, sentence as value\n",
    "    returns dictionary with the index as key, sentenece mapped to index as value, and padded to pad_len\n",
    "    \n",
    "    QUESTION: How should we choose pad_len?  Should we truncate or should we set to the max length of \n",
    "    premise and hypothesis?\n",
    "    '''\n",
    "    tokenized_data = {}\n",
    "    for i in range(len(text_dict.keys())):\n",
    "        text_list = text_dict[i].split()\n",
    "        clean_words(text_list)\n",
    "        text_idx = []\n",
    "        for word in text_list:\n",
    "            try:\n",
    "                text_idx.append(idx_mapping[word])\n",
    "            except KeyError:\n",
    "                #UNK token\n",
    "                text_idx.append(1)\n",
    "                continue\n",
    "        #insert BoS token\n",
    "        text_idx.insert(0,2)\n",
    "        if len(text_idx) > pad_len:\n",
    "            text_idx = text_idx[:pad_len]\n",
    "        text_idx = np.concatenate((text_idx,np.zeros(max(pad_len-len(text_idx),0))))\n",
    "                                    \n",
    "        tokenized_data[i] = np.array(text_idx).astype(int)\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for lisa's laptop\n",
    "vocab_size = 50000\n",
    "emb_dim = 300\n",
    "num_classes = 3\n",
    "learning_rate = .001\n",
    "path = '/Users/Lisa/Documents/Grad School/DS-GA 1101/data/'\n",
    "glove_path = path+'glove.6B/glove.6B.300d.txt'\n",
    "text_path = path+'snli_1.0/snli_1.0_train.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "emb_dim = 300\n",
    "num_classes = 3\n",
    "#do we use learning rate anywhere?\n",
    "learning_rate = .001\n",
    "glove_path = 'glove/glove.6B.300d.txt'\n",
    "text_path = 'snli_1.0/snli_1.0_train.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785 excluded\n"
     ]
    }
   ],
   "source": [
    "hypothesis, premise, label, label_enc = load_data(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, words, idx2words, ordered_words = load_embeddings(glove_path, vocab_size, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 300)\n",
      "(50001, 300)\n",
      "(50002, 300)\n",
      "(50003, 300)\n"
     ]
    }
   ],
   "source": [
    "#modifies embeddings, words, idx2words in place to add tokens\n",
    "words, embeddings = add_tokens(words, embeddings, emb_dim)\n",
    "idx2words = {v:k for k,v in words.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_len = 10\n",
    "p_len = 15\n",
    "h_idx = tokenize(hypothesis, words, h_len)\n",
    "p_idx = tokenize(premise, words, p_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = path+'pickles/'\n",
    "with open(fp+'h_idx.pt', 'wb') as f:\n",
    "    pickle.dump(h_idx, f)\n",
    "with open(fp+'p_idx.pt', 'wb') as f:\n",
    "    pickle.dump(p_idx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample indices\n",
    "idx_list = np.array((0,1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_test_batch = torch.LongTensor(4, h_len)\n",
    "p_test_batch = torch.LongTensor(4, p_len)\n",
    "l_test_batch = torch.LongTensor(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test batch\n",
    "for i in idx_list:\n",
    "    h_test_batch[i] = torch.from_numpy(h_idx[i])\n",
    "    p_test_batch[i] = torch.from_numpy(p_idx[i])\n",
    "    l_test_batch[i] = label_enc[i]\n",
    "h_test_batch = h_test_batch.long()\n",
    "p_test_batch = p_test_batch.long()\n",
    "l_test_batch = Variable(l_test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecomposableAttention(nn.Module):\n",
    "    '''\n",
    "    Starting with premise (a), we see if the hypothesis (b) is an \n",
    "    entailment, a contradiction, or neutral.\n",
    "    '''\n",
    "    def __init__(self, glove_emb, batch_size, hidden_size, h_len, p_len, num_classes, dropout=0.2):\n",
    "        super(DecomposableAttention, self).__init__()\n",
    "        self.glove = glove_emb\n",
    "        self.num_embeddings = glove_emb.shape[0]\n",
    "        self.embedding_dim = glove_emb.shape[1]\n",
    "        self.batch_size = batch_size\n",
    "        self.compare_dim = 2*self.embedding_dim\n",
    "        self.aggregate_dim = 2*self.compare_dim\n",
    "        self.h_len = h_len\n",
    "        self.p_len = p_len\n",
    "        \n",
    "        self.embed = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        \n",
    "        '''\n",
    "        Should the linear layers have a bias?  How many layers should we have in our feed forward network?\n",
    "        Yes?\n",
    "        '''\n",
    "        \n",
    "        self.F_a = nn.Linear(self.embedding_dim*self.p_len,self.embedding_dim*self.p_len)\n",
    "        self.F_b = nn.Linear(self.embedding_dim*self.h_len,self.embedding_dim*self.h_len)\n",
    "        \n",
    "        self.G_a = nn.Linear(self.embedding_dim*2,self.compare_dim)\n",
    "        self.G_b = nn.Linear(self.embedding_dim*2,self.compare_dim)\n",
    "        \n",
    "        self.H = nn.Linear(self.compare_dim,self.aggregate_dim)\n",
    "        self.output = nn.Linear(self.aggregate_dim,3)\n",
    "       \n",
    "        self.init_weights()\n",
    "    \n",
    "    def _mlp_layers(self, input_dim, output_dim):\n",
    "        mlp_layers = []\n",
    "        mlp_layers.append(nn.Dropout(p=0.2))\n",
    "        mlp_layers.append(nn.Linear(\n",
    "            input_dim, output_dim, bias=True))\n",
    "        mlp_layers.append(nn.ReLU())\n",
    "        mlp_layers.append(nn.Dropout(p=0.2))\n",
    "        mlp_layers.append(nn.Linear(\n",
    "            output_dim, output_dim, bias=True))\n",
    "        mlp_layers.append(nn.ReLU())   \n",
    "        #sequential runs all the layers in order\n",
    "        return nn.Sequential(*mlp_layers)  \n",
    "    \n",
    "    def forward(self, hypothesis, premise, label):\n",
    "        start_time = time.time()\n",
    "        '''\n",
    "        Embedding layer\n",
    "        max length = max length of of hypothesis/premise (respectively) in batch\n",
    "        Input dim: batch size x max length\n",
    "        Output dim: batch size x max length x embedding dimensions\n",
    "        '''\n",
    "        p_embedded = self.embed(Variable(premise))\n",
    "        h_embedded = self.embed(Variable(hypothesis))\n",
    "        \n",
    "        '''\n",
    "        Relu layer (F from paper)\n",
    "        max length = max length of of hypothesis/premise (respectively) in batch\n",
    "        Input dim: batch size x max length x embedding dimensions\n",
    "        Output dim: batch size x max length x embedding dimensions\n",
    "        '''\n",
    "        F_a = self.F_a(p_embedded.view(self.batch_size,-1))\n",
    "        F_b = self.F_b(h_embedded.view(self.batch_size,-1))\n",
    "        F_a = F.relu(F_a).view(self.batch_size,-1,self.embedding_dim)\n",
    "        F_b = F.relu(F_b).view(self.batch_size,-1,self.embedding_dim)\n",
    "        #E dim: batch_size x max len of hypothesis x max len of premise\n",
    "        #transpose function swaps second and third axis so that F_b is batch size x embedding dim x len premise\n",
    "        E = torch.matmul(F_a,torch.transpose(F_b,1,2))  \n",
    "        \n",
    "        '''\n",
    "        Attention! \n",
    "        Given E, we reweight using the softmax and store in W_beta, W_alpha\n",
    "        W_beta dim: batch_size x len(hypothesis) x embedding dimensions\n",
    "        W_alpha dim: batch_size x len(premise) x embedding dimensions\n",
    "        '''\n",
    "        W_beta = Variable(torch.Tensor(self.batch_size,self.p_len,self.embedding_dim))\n",
    "        W_alpha = Variable(torch.Tensor(self.batch_size,self.h_len,self.embedding_dim))\n",
    "        '''\n",
    "        TO DO: vectorize this with softmax on dimension (should be in next release of pytorch)\n",
    "        '''\n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(F_b.size()[1]):\n",
    "                W_beta[i,j] = torch.mm(F.softmax(E[i,j]).view(1,-1),h_embedded[i]).data\n",
    "            for k in range(F_a.size()[1]):\n",
    "                W_alpha[i,j] = torch.mm(F.softmax(E[i,:,j]).view(1,-1),p_embedded[i]).data\n",
    "        \n",
    "        '''\n",
    "        Compare\n",
    "        Open items:\n",
    "        1) Check that we're concatenating along the right dimensions.  Based on AllenNLP and libowen, \n",
    "            concatenated input should be batch size x len(hypothesis/premise) x (2 * embedding dim)\n",
    "        \n",
    "        Output:\n",
    "        v1 dim: batch_size x len(hypothesis) x compare_dim\n",
    "        v2 dim: batch_size x len(premise) x compare_dim\n",
    "        '''\n",
    "        #dim: batch size x len(hypotheis/premise) x (2* embedding dim)\n",
    "        cat_p_beta = torch.cat((p_embedded,W_beta),2)\n",
    "        cat_h_alpha = torch.cat((h_embedded,W_alpha),2)\n",
    "        G_a = self.G_a(cat_p_beta.view(-1,2*self.embedding_dim)).view(self.batch_size,-1,self.compare_dim)\n",
    "        G_b = self.G_b(cat_h_alpha.view(-1,2*self.embedding_dim)).view(self.batch_size,-1,self.compare_dim)\n",
    "        \n",
    "        v_a = F.relu(G_a).view(self.batch_size,-1,self.compare_dim)\n",
    "        v_b = F.relu(G_b).view(self.batch_size,-1,self.compare_dim)\n",
    "        \n",
    "        '''\n",
    "        Aggregate\n",
    "        Given:\n",
    "        v_a = output of relu activation on the concatenation of a (premise) and beta\n",
    "        v_b = output of relu activation on the concatenation of b (hypothesis) and alpha\n",
    "        '''\n",
    "        v1 = torch.sum(v_a, dim=1)\n",
    "        v2 = torch.sum(v_b, dim=1)\n",
    "        H = F.relu(torch.cat((v1,v2),1))\n",
    "        out = F.softmax(self.output(H))\n",
    "        \n",
    "        print(\"runtime for single batch: %s seconds\" % (time.time() - start_time))\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(self.glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "dropout_rate = .2\n",
    "batch_size = 4\n",
    "hidden_size = 100\n",
    "h_len = 10\n",
    "p_len = 15\n",
    "da = DecomposableAttention(embeddings,batch_size,hidden_size,\\\n",
    "                           h_len,p_len,num_classes,dropout=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(da.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime for single batch: 0.02220892906188965 seconds\n"
     ]
    }
   ],
   "source": [
    "'''This should go in the training loop once our batch iterator is working:'''\n",
    "optimizer.zero_grad()\n",
    "output = da(h_test_batch,p_test_batch,l_test_batch)\n",
    "loss = criterion(output,l_test_batch.view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.6252\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
